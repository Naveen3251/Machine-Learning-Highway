{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347fdc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1787eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    L : Length of input sequence (eg: \"My name is Naveen\")\n",
    "    d_k : dimension of key vector\n",
    "'''\n",
    "L,d_k=4,8\n",
    "\n",
    "'''\n",
    "Query (Q): Represents the token for which attention scores are computed.\n",
    "Key (K): Represents tokens against which similarity is measured relative to the query.\n",
    "Value (V): Represents the content the model should focus on based on the attention scores.\n",
    "'''\n",
    "Q=np.random.randn(L,d_k)\n",
    "K=np.random.randn(L,d_k)\n",
    "V=np.random.randn(L,d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9108feb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q [[-0.12952816  0.4968856  -1.11360572  0.64376398  0.68393544  1.71629992\n",
      "  -0.46210517 -2.06693499]\n",
      " [ 1.62719989  0.12665391  1.33433868  0.60299945 -2.09292818  1.35875407\n",
      "  -1.02141623 -0.69122584]\n",
      " [ 0.47803292 -0.87641853  0.69799344 -0.55551601  0.55171491 -0.43617022\n",
      "  -1.82168364 -0.079966  ]\n",
      " [-1.47421489  0.56718449 -1.0868938  -0.48990292  0.64663416 -0.55543494\n",
      "  -0.21241774  2.364312  ]]\n",
      "K [[ 0.14817456  1.03362618 -0.32496023 -0.44147382 -0.15858248  1.39706859\n",
      "   0.82240463 -0.06290707]\n",
      " [-0.78722209 -0.39043776 -0.99504845  0.6633271  -0.17409011 -0.62540127\n",
      "   0.34685703  0.33057644]\n",
      " [-1.56385772 -0.17226573 -1.46686369  1.15585957  0.68717778  0.95540222\n",
      "   0.50328574 -2.53532054]\n",
      " [ 0.25841468  0.58202733  0.17330716  0.84707466 -0.08903169 -1.97877716\n",
      "  -1.35320251  0.32104006]]\n",
      "V [[ 0.67190478 -1.85280841 -0.06877835  1.03676013 -1.22125968 -0.27264151\n",
      "  -1.14697016  0.4664111 ]\n",
      " [ 0.70596376  0.61012559  0.26463287  0.4541557   0.10243701  0.43204941\n",
      "   1.40552108  1.3816185 ]\n",
      " [ 0.52038012  1.10961589 -1.46916552  1.0729029  -0.1476447   0.2674718\n",
      "  -2.10049557 -1.03714367]\n",
      " [-1.51861513 -0.29589992 -0.49066089 -0.62525459 -0.44392162  0.27320102\n",
      "  -1.39895999  0.14297269]]\n"
     ]
    }
   ],
   "source": [
    "print('Q',Q)\n",
    "print('K',K)\n",
    "print('V',V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d2cd6c",
   "metadata": {},
   "source": [
    "### Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26f0df",
   "metadata": {},
   "source": [
    "$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}} + \\text{Mask} \\right) \\mathbf{V} \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64db7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled=np.matmul(Q,K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6ea0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.61138973, -0.59292471,  9.61208958, -2.88726473],\n",
       "       [ 1.10584688, -3.32636072, -2.72847974, -0.10581839],\n",
       "       [-3.00661381, -1.57872265, -3.01423804,  2.51723308],\n",
       "       [-0.26465864,  2.63833476, -2.95168475,  1.43380138]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bac3f",
   "metadata": {},
   "source": [
    "### 1] Why we need sqrt(d_k) in denominator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d256a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varinace of Q:  1.194145607051731\n",
      "Varinace of K:  0.8649907289769572\n",
      "Varinace of QK_T:  10.773626616673676\n"
     ]
    }
   ],
   "source": [
    "print(\"Varinace of Q: \",Q.var())\n",
    "print(\"Varinace of K: \",K.var())\n",
    "print(\"Varinace of QK_T: \",np.matmul(Q,K.T).var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a22263",
   "metadata": {},
   "source": [
    "#### NOTE : Since varinace is so larger so we need to use sqrt(dk) in denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3c7f4",
   "metadata": {},
   "source": [
    "### 2] Scaled vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ba3dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92326569, -0.20963054,  3.39838686, -1.02080224],\n",
       "       [ 0.39097592, -1.17604611, -0.96466326, -0.03741245],\n",
       "       [-1.06299851, -0.55816275, -1.06569408,  0.88997629],\n",
       "       [-0.09357096,  0.9327922 , -1.04357815,  0.50692534]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Q,K.T)/np.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d3065",
   "metadata": {},
   "source": [
    "### 3] Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1227073",
   "metadata": {},
   "source": [
    "### Masking in Transformer Architecture\n",
    "\n",
    "In transformers, **masking is crucial for ensuring that attention mechanisms operate correctly by controlling which parts of the input sequences can be attended to during computation**. \n",
    "\n",
    "#### Types of Masks\n",
    "\n",
    "1. **Padding Mask (Both Encoder and Decoder):**\n",
    "   - **Purpose:** Handles variable-length sequences by masking padding tokens.\n",
    "   - **Implementation:** Assigns a mask value of 0 to padding tokens and 1 to actual tokens.\n",
    "\n",
    "2. **Self-Attention Mask (Encoder - Optional for bidirectional models, Decoder):**\n",
    "   - **Purpose:** Prevents tokens from attending to future tokens.\n",
    "   - **Implementation:** Creates an upper triangular matrix of 1s (allowing self-attention only to previous tokens) and 0s below the diagonal.\n",
    "\n",
    "3. **Causal Mask (Decoder):**\n",
    "   - **Purpose:** Maintains the autoregressive property during training.\n",
    "   - **Implementation:** Creates a lower triangular matrix of 1s (allowing attention only to itself and previous tokens) and 0s above the diagonal.\n",
    "\n",
    "4. **Cross-Attention Mask (Decoder):**\n",
    "   - **Purpose:** Controls how decoder tokens attend to encoder outputs.\n",
    "   - **Implementation:** Typically allows full attention to all encoder outputs, but can be customized based on specific requirements.\n",
    "\n",
    "#### Combined Masking Example\n",
    "\n",
    "Let's consider an example where we have a batch of sequences with variable lengths and we want to apply masking in a transformer model. Suppose we have two sequences:\n",
    "\n",
    "- Sequence 1: [5, 6, 0, 0, 0]\n",
    "- Sequence 2: [3, 4, 7, 0, 0]\n",
    "\n",
    "##### Step-by-Step Masking\n",
    "\n",
    "1. **Padding Mask Creation:**\n",
    "\n",
    "   Create a padding mask for each sequence:\n",
    "\n",
    "   - Sequence 1: [1, 1, 0, 0, 0]\n",
    "   - Sequence 2: [1, 1, 1, 0, 0]\n",
    "\n",
    "   Here, 1 denotes actual tokens, and 0 denotes padding tokens.\n",
    "\n",
    "2. **Encoder Self-Attention Mask (Optional for bidirectional models):**\n",
    "\n",
    "   Create an upper triangular matrix for each sequence (considering maximum sequence length in batch):\n",
    "\n",
    "   - Sequence 1: \n",
    "     ```\n",
    "     [1, 1, 1, 0, 0]\n",
    "     [0, 1, 1, 0, 0]\n",
    "     [0, 0, 1, 0, 0]\n",
    "     [0, 0, 0, 1, 0]\n",
    "     [0, 0, 0, 0, 1]\n",
    "     ```\n",
    "\n",
    "   - Sequence 2:\n",
    "     ```\n",
    "     [1, 1, 1, 0, 0]\n",
    "     [0, 1, 1, 0, 0]\n",
    "     [0, 0, 1, 0, 0]\n",
    "     [0, 0, 0, 1, 0]\n",
    "     [0, 0, 0, 0, 1]\n",
    "     ```\n",
    "\n",
    "   Here, the upper triangular matrix ensures that each token attends only to itself and previous tokens.\n",
    "\n",
    "3. **Decoder Causal Mask:**\n",
    "\n",
    "   Create a lower triangular matrix for each sequence (considering maximum sequence length in batch):\n",
    "\n",
    "   - Sequence 1:\n",
    "     ```\n",
    "     [1, 0, 0, 0, 0]\n",
    "     [1, 1, 0, 0, 0]\n",
    "     [1, 1, 1, 0, 0]\n",
    "     [1, 1, 1, 1, 0]\n",
    "     [1, 1, 1, 1, 1]\n",
    "     ```\n",
    "\n",
    "   - Sequence 2:\n",
    "     ```\n",
    "     [1, 0, 0, 0, 0]\n",
    "     [1, 1, 0, 0, 0]\n",
    "     [1, 1, 1, 0, 0]\n",
    "     [1, 1, 1, 1, 0]\n",
    "     [1, 1, 1, 1, 1]\n",
    "     ```\n",
    "\n",
    "   Here, the lower triangular matrix ensures that each token in the decoder attends only to itself and previous tokens in the target sequence.\n",
    "\n",
    "4. **Cross-Attention Mask (Decoder):**\n",
    "\n",
    "   Typically, this mask allows full attention to all encoder outputs, so it might look like this (assuming full attention):\n",
    "\n",
    "   - Sequence 1 to Encoder: [1, 1, 1, 1, 1]\n",
    "   - Sequence 2 to Encoder: [1, 1, 1, 1, 1]\n",
    "\n",
    "   Here, each token in the decoder can attend to all tokens in the encoder outputs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff5654",
   "metadata": {},
   "source": [
    "### 4] Casual mask (Lower Triangular mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4f5f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1384d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0]=-np.inf\n",
    "mask[mask==1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1865c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3b33a",
   "metadata": {},
   "source": [
    "### 5] Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4bacc",
   "metadata": {},
   "source": [
    "Converting vector to probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa9e90",
   "metadata": {},
   "source": [
    "$\\text{softmax}(\\mathbf{Z})_i = \\frac{e^{Z_i}}{\\sum_j e^{Z_j}}\\$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a133023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.61138973,        -inf,        -inf,        -inf],\n",
       "       [ 1.10584688, -3.32636072,        -inf,        -inf],\n",
       "       [-3.00661381, -1.57872265, -3.01423804,        -inf],\n",
       "       [-0.26465864,  2.63833476, -2.95168475,  1.43380138]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04ba6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314d7e7",
   "metadata": {},
   "source": [
    "### Result with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acef3c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.98825145, 0.01174855, 0.        , 0.        ],\n",
       "       [0.16227704, 0.67667844, 0.16104451, 0.        ],\n",
       "       [0.04038407, 0.73614632, 0.00274947, 0.22072013]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314edfd",
   "metadata": {},
   "source": [
    "Note : here mask helps to hide the future words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd556051",
   "metadata": {},
   "source": [
    "### Result without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a526e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.10377374e-04, 3.69492285e-05, 9.99048948e-01, 3.72551369e-06],\n",
       "       [7.51198978e-01, 8.93041583e-03, 1.62378339e-02, 2.23632772e-01],\n",
       "       [3.89469512e-03, 1.62404747e-02, 3.86511398e-03, 9.75999716e-01],\n",
       "       [4.03840741e-02, 7.36146321e-01, 2.74947337e-03, 2.20720131e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bea22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention=softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24101b",
   "metadata": {},
   "source": [
    "### New value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78cfe402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67190478, -1.85280841, -0.06877835,  1.03676013, -1.22125968,\n",
       "        -0.27264151, -1.14697016,  0.4664111 ],\n",
       "       [ 0.67230492, -1.82387251, -0.06486125,  1.02991537, -1.20570817,\n",
       "        -0.26436242, -1.1169821 ,  0.47716346],\n",
       "       [ 0.67054954,  0.29088811, -0.06869083,  0.64834487, -0.15264286,\n",
       "         0.29118993,  0.4266856 ,  0.84357297],\n",
       "       [ 0.21306872,  0.31205755,  0.0796928 ,  0.24113729, -0.07229919,\n",
       "         0.36807758,  0.67379595,  1.06461431]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_V=np.matmul(attention,V)\n",
    "new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085a98d",
   "metadata": {},
   "source": [
    "# Combined code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e611c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k=q.shape[-1]\n",
    "    scaled=np.matmul(q,k.T)/np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled=scaled+mask\n",
    "    attention=softmax(scaled)\n",
    "    out=np.matmul(attention,v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12477cd0",
   "metadata": {},
   "source": [
    "### At decoder with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f007ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At decoder with mask\n",
      "Attention\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.82735866 0.17264134 0.         0.        ]\n",
      " [0.27367108 0.45339455 0.27293437 0.        ]\n",
      " [0.16664837 0.46509848 0.0644493  0.30380386]]\n",
      "ouput\n",
      "[[ 0.67190478 -1.85280841 -0.06877835  1.03676013 -1.22125968 -0.27264151\n",
      "  -1.14697016  0.4664111 ]\n",
      " [ 0.67778476 -1.42760418 -0.01121779  0.93617852 -0.99273491 -0.15098273\n",
      "  -0.70630465  0.62441373]\n",
      " [ 0.64599065  0.07241986 -0.29982531  0.78247507 -0.32807639  0.194277\n",
      "  -0.24993441  0.47098936]\n",
      " [ 0.01249149 -0.04338059 -0.13213281  0.26319459 -0.30025833  0.25574815\n",
      "  -0.0978199   0.69690778]]\n"
     ]
    }
   ],
   "source": [
    "output,attention=scaled_dot_product_attention(Q,K,V,mask)\n",
    "print(\"At decoder with mask\")\n",
    "print('Attention')\n",
    "print(attention)\n",
    "print('ouput')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb59ba",
   "metadata": {},
   "source": [
    "### At encoder mask is optional mostly we wont use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb52134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At decoder with mask\n",
      "Attention\n",
      "[[0.07491553 0.02413022 0.89023229 0.01072195]\n",
      " [0.47214209 0.09851984 0.12170996 0.30762811]\n",
      " [0.09342608 0.15478024 0.09317458 0.65861909]\n",
      " [0.16664837 0.46509848 0.0644493  0.30380386]]\n",
      "ouput\n",
      "[[ 0.51434784  0.86056161 -1.31192635  1.03705718 -0.22521728  0.23104164\n",
      "  -1.93693892 -0.85348564]\n",
      " [-0.0170473  -0.7706552  -0.33615472  0.47247852 -0.72104862  0.0304382\n",
      "  -1.08907182  0.27408086]\n",
      " [-0.77966004 -0.17016258 -0.42551327 -0.14468256 -0.40437427  0.24625786\n",
      "  -1.00670459  0.25495132]\n",
      " [ 0.01249149 -0.04338059 -0.13213281  0.26319459 -0.30025833  0.25574815\n",
      "  -0.0978199   0.69690778]]\n"
     ]
    }
   ],
   "source": [
    "output,attention=scaled_dot_product_attention(Q,K,V)\n",
    "print(\"At decoder with mask\")\n",
    "print('Attention')\n",
    "print(attention)\n",
    "print('ouput')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cfeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
