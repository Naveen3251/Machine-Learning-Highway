{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b115cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c533ae",
   "metadata": {},
   "source": [
    "# Numerical representation of our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b05d5",
   "metadata": {},
   "source": [
    "Sure, let's represent the operations in the code using equations:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - **Input Layer to Hidden Layer:**\n",
    "     - **Matrix Multiplication:** $\\ z^{[1]} = x \\cdot W^{[1]} + b^{[1]} \\$\n",
    "     \n",
    "     - **Activation Function:** $\\ a^{[1]} = \\sigma(z^{[1]}) \\$\n",
    "     \n",
    "   - **Hidden Layer to Output Layer:**\n",
    "     - **Matrix Multiplication:** $\\ z^{[2]} = a^{[1]} \\cdot W^{[2]} + b^{[2]} \\$\n",
    "     \n",
    "     - **Activation Function:**  $\\hat{y} = a^{[2]} = \\sigma(z^{[2]}) \\$\n",
    "   \n",
    "2. **Loss Computation:**\n",
    "   - **Mean Squared Error Loss:** $\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 \\$\n",
    "\n",
    "3. **Backpropagation:**\n",
    "   - **Compute Gradients:**\n",
    "     \n",
    "   - **Update Weights and Biases:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597e916",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "x & \\text{ represents the input data,} \\\\\n",
    "W^{[1]} & \\text{ represents the weights of the first layer,} \\\\\n",
    "W^{[2]} & \\text{ represents the weights of the second layer,} \\\\\n",
    "b^{[1]} & \\text{ represents the biases of the first layer,} \\\\\n",
    "b^{[2]} & \\text{ represents the biases of the second layer,} \\\\\n",
    "\\sigma & \\text{ represents the sigmoid activation function,} \\\\\n",
    "\\hat{y} & \\text{ represents the predicted output,} \\\\\n",
    "y & \\text{ represents the true labels,} \\\\\n",
    "m & \\text{ represents the number of samples,} \\\\\n",
    "\\alpha & \\text{ represents the learning rate.}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adf85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####code####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b91ee",
   "metadata": {},
   "source": [
    "# Building the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be5a91",
   "metadata": {},
   "source": [
    "### Training the neural network for performing XOR operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "663b7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    '''\n",
    "        x : input\n",
    "    '''\n",
    "    #hidden layer\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    a1=tf.nn.sigmoid(z1) #binary classification thus sigmoid activation\n",
    "    \n",
    "    #ouput layer\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)#binary classification thus sigmoid activation\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function \n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    '''\n",
    "        y_true : True label\n",
    "        y_pred : Predicted label\n",
    "        \n",
    "        MSE : mean square error\n",
    "        \n",
    "    '''\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "      \n",
    "# Optimizer - Stochastic gradient optimizer\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "#Training step\n",
    "@tf.function\n",
    "def train_step(x,y):\n",
    "    '''\n",
    "     here we use , \n",
    "     Gradient Tape (Auto differentiation) : Which is used for backpropagation\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        '''\n",
    "        TensorFlow automatically tracks gradients for trainable variables (weights and biases).\n",
    "        Thus we didnt use watch\n",
    "        '''\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "        \n",
    "    #gradient calculation   \n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46ae8a",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b54498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.13042792677879333\n",
      "Epoch 1000, Loss: 0.12999320030212402\n",
      "Epoch 2000, Loss: 0.1296095848083496\n",
      "Epoch 3000, Loss: 0.12926629185676575\n",
      "Epoch 4000, Loss: 0.1289544403553009\n",
      "Epoch 5000, Loss: 0.12866628170013428\n",
      "Epoch 6000, Loss: 0.12839487195014954\n",
      "Epoch 7000, Loss: 0.12813347578048706\n",
      "Epoch 8000, Loss: 0.1278754621744156\n",
      "Epoch 9000, Loss: 0.12761344015598297\n",
      "Epoch 10000, Loss: 0.12733928859233856\n",
      "Epoch 11000, Loss: 0.12704312801361084\n",
      "Epoch 12000, Loss: 0.1267128586769104\n",
      "Epoch 13000, Loss: 0.12633317708969116\n",
      "Epoch 14000, Loss: 0.1258845031261444\n",
      "Epoch 15000, Loss: 0.12534154951572418\n",
      "Epoch 16000, Loss: 0.12467201799154282\n",
      "Epoch 17000, Loss: 0.12383563816547394\n",
      "Epoch 18000, Loss: 0.12278340756893158\n",
      "Epoch 19000, Loss: 0.12145747244358063\n",
      "Epoch 20000, Loss: 0.1197902262210846\n",
      "Epoch 21000, Loss: 0.11770264804363251\n",
      "Epoch 22000, Loss: 0.11510228365659714\n",
      "Epoch 23000, Loss: 0.11188478767871857\n",
      "Epoch 24000, Loss: 0.1079433336853981\n",
      "Epoch 25000, Loss: 0.10318626463413239\n",
      "Epoch 26000, Loss: 0.09756182134151459\n",
      "Epoch 27000, Loss: 0.09108518064022064\n",
      "Epoch 28000, Loss: 0.08386191725730896\n",
      "Epoch 29000, Loss: 0.0760975331068039\n",
      "Epoch 30000, Loss: 0.06808125972747803\n",
      "Epoch 31000, Loss: 0.06014325097203255\n",
      "Epoch 32000, Loss: 0.052593499422073364\n",
      "Epoch 33000, Loss: 0.045670293271541595\n",
      "Epoch 34000, Loss: 0.03951318562030792\n",
      "Epoch 35000, Loss: 0.034165866672992706\n",
      "Epoch 36000, Loss: 0.029599536210298538\n",
      "Epoch 37000, Loss: 0.02574160136282444\n",
      "Epoch 38000, Loss: 0.022500425577163696\n",
      "Epoch 39000, Loss: 0.01978190988302231\n",
      "Epoch 40000, Loss: 0.017499050125479698\n",
      "Epoch 41000, Loss: 0.015575980767607689\n",
      "Epoch 42000, Loss: 0.013948678970336914\n",
      "Epoch 43000, Loss: 0.012564396485686302\n",
      "Epoch 44000, Loss: 0.011380139738321304\n",
      "Epoch 45000, Loss: 0.010361026972532272\n",
      "Epoch 46000, Loss: 0.009478935971856117\n",
      "Epoch 47000, Loss: 0.008710982277989388\n",
      "Epoch 48000, Loss: 0.008038705214858055\n",
      "Epoch 49000, Loss: 0.00744705181568861\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 50000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2c494",
   "metadata": {},
   "source": [
    "# Testing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9436e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the network:\n",
      "Input: [0, 0], Output: [[0.11293911]]\n",
      "Input: [0, 1], Output: [[0.8031352]]\n",
      "Input: [1, 0], Output: [[0.98742217]]\n",
      "Input: [1, 1], Output: [[0.04208273]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the network:\")\n",
    "print(\"Input: [0, 0], Output:\", forward_pass(tf.constant([[0.0, 0.0]], dtype=tf.float32)).numpy())\n",
    "print(\"Input: [0, 1], Output:\", forward_pass(tf.constant([[0.0, 1.0]], dtype=tf.float32)).numpy())\n",
    "print(\"Input: [1, 0], Output:\", forward_pass(tf.constant([[1.0, 0.0]], dtype=tf.float32)).numpy())\n",
    "print(\"Input: [1, 1], Output:\", forward_pass(tf.constant([[1.0, 1.0]], dtype=tf.float32)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Above is the probability that likely to 1 and 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
