{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30c4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3f635",
   "metadata": {},
   "source": [
    "# Eager execution"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652aa256",
   "metadata": {},
   "source": [
    "Eager execution is a feature in TensorFlow that allows operations to be executed immediately as they are called from Python. \n",
    "This makes TensorFlow easier to use and debug because it operates more like standard Python, rather than \n",
    "requiring the construction of computational graphs that are executed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc7976",
   "metadata": {},
   "source": [
    "### Example illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad80030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tf.Tensor(5, shape=(), dtype=int32)\n",
      "a * b = tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant(2)\n",
    "b=tf.constant(3)\n",
    "add=a+b\n",
    "mul=a*b\n",
    "print(\"a + b =\",add)\n",
    "print(\"a * b =\",mul)\n",
    "\n",
    "#In this example, a + b and a * b are executed immediately, and their results are available as soon as they are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654eca67",
   "metadata": {},
   "source": [
    "# Function/Graph - tf.function()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaf57d9b",
   "metadata": {},
   "source": [
    "tf.function is a powerful decorator in TensorFlow that allows you to convert a Python function into a TensorFlow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433bbbd",
   "metadata": {},
   "source": [
    "**Performance Optimization:** Graphs are optimized for performance, allowing for faster execution compared to eager mode.\n",
    "**Serialization:** Graphs can be serialized, which is useful for saving and deploying models.\n",
    "**Device Placement:** Graphs can be executed on various devices (CPU, GPU, TPU) without modification.\n",
    "**Parallel Execution:** Operations in the graph can be parallelized and optimized by the TensorFlow runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f7ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resukt =  tf.Tensor(50, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#example illustartion\n",
    "@tf.function\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "x,y=20,30\n",
    "res=add(x,y)\n",
    "print(\"resukt = \",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdba8f2",
   "metadata": {},
   "source": [
    "# GradientTape() - Automatic diffrentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24eb34",
   "metadata": {},
   "source": [
    "**tf.GradientTape:** This context manager records operations for automatic differentiation.\n",
    "\n",
    "**tape.watch(tensor):** Manually watches a tensor. This is necessary for tensors that are not tf.Variable objects, such as constants.\n",
    "\n",
    "**tape.gradient(target, sources):** Computes the gradient of target (usually a loss function) with respect to sources (usually model parameters).\n",
    "\n",
    "**tape.stop_recording():** Temporarily stops recording operations.\n",
    "\n",
    "**tape.reset():** Resets the tape, discarding all recorded operations.\n",
    "\n",
    "**tape.enter()** and **tape.exit(exc_type, exc_value, traceback):** These methods are used internally to start and stop the gradient recording when entering and exiting the context manager.\n",
    "\n",
    "**tape.batch_jacobian(target, source):** Computes the Jacobian matrix of target with respect to source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d5328",
   "metadata": {},
   "source": [
    "### Example-1 - with constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876761a9",
   "metadata": {},
   "source": [
    "Before diff : f(x)=x^2+3x+2\n",
    "\n",
    "After diff with respect to x : f'(x)=2x+3\n",
    "\n",
    "At x=2 : \n",
    "\n",
    "f(2.0)=16\n",
    "\n",
    "f'(2.0)=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6821a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x)= tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "f'(x)= tf.Tensor(15.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define a function for which we want to compute the gradient\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return x**3+3*x+2\n",
    "\n",
    "# Define the variable\n",
    "# Note: that this is a tf.Tensor object, not a tf.Variable.\n",
    "x=tf.constant(2.0)\n",
    "\n",
    "\"\"\"\n",
    "with tf.GradientTape() as tape: This context manager starts recording operations.\n",
    "tape.watch(x): Explicitly tells the tape to watch the tensor x for gradient computation.\n",
    "y = f(x): Computes the function value \n",
    "\"\"\"\n",
    "@tf.function\n",
    "def compute_gradient(x):\n",
    "    \n",
    "    #This context manager starts recording operations\n",
    "    with tf.GradientTape() as tape: \n",
    "        tape.watch(x) # Manually watch x because it's not a tf.Variable\n",
    "        y=f(x) # Define the function\n",
    "        \n",
    "    # Compute the gradient of y with respect to x\n",
    "    return tape.gradient(y,x)\n",
    "\n",
    "f_x=f(x)\n",
    "dy_dx=compute_gradient(x)\n",
    "\n",
    "print(\"f(x)=\",f_x)\n",
    "print(\"f'(x)=\",dy_dx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf312272",
   "metadata": {},
   "source": [
    "### Example 2-  with variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e4247",
   "metadata": {},
   "source": [
    "y=w*x+b\n",
    "\n",
    "\n",
    "dy_dw=x\n",
    "\n",
    "dy_db=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb595984",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(2.0)\n",
    "b = tf.Variable(3.0)\n",
    "x = tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe15bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to w: 1.0\n",
      "Gradient of y with respect to b: 1.0\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    return w*x+b\n",
    "@tf.function\n",
    "def compute_gradient(x):\n",
    "    #This context manager starts recording operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        #define function\n",
    "        y=f(x)\n",
    "    return tape.gradient(y,[w,b])\n",
    "\n",
    "gradients=compute_gradient(x)\n",
    "print(\"Gradient of y with respect to w:\", gradients[0].numpy())\n",
    "print(\"Gradient of y with respect to b:\", gradients[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b404d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note :  I above example we didnt use the tape.watch(x) \n",
    "#reason:\n",
    "#dy_dw and dy_db we are computing these two are declared with tf.Variable() so dont have to use that watch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3d9f2",
   "metadata": {},
   "source": [
    "### Example 3 - Temporarily stop recording operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35f81c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient when recording was stopped: None\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant(2.0)\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return x**2+3*x+2\n",
    "\n",
    "@tf.function\n",
    "def compute_gradient(x):\n",
    "    #This context manager starts recording operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        with tape.stop_recording():\n",
    "            y=f(x) # This won't be recorded\n",
    "    # Compute gradient (will be None because recording was stopped)\n",
    "    return tape.gradient(y,x)\n",
    "\n",
    "dy_dx=compute_gradient(x)\n",
    "print(\"Gradient when recording was stopped:\", dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babea96b",
   "metadata": {},
   "source": [
    "### Example 4 - Compute Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "103af466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      " [[[2. 4.]]\n",
      "\n",
      " [[6. 8.]]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "@tf.function\n",
    "def compute_jacobian(x):\n",
    "     #This context manager starts recording operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        #defining the function\n",
    "        y=tf.reduce_sum(x**2,axis=1)\n",
    "        y = tf.reshape(y, (-1, 1))\n",
    "    #computing jacobian\n",
    "    return tape.batch_jacobian(y,x)\n",
    "\n",
    "jacob=compute_jacobian(x)\n",
    "print(\"Jacobian matrix:\\n\", jacob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0e559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
