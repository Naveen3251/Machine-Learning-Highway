{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aa172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd5cab",
   "metadata": {},
   "source": [
    "# 1] SGD-Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87f12a",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Simplifies the training process by updating model parameters using the gradients of the loss function with respect to the entire training dataset or a batch of data points.\n",
    "  - Effective for convex and smooth optimization problems, where the objective is to minimize the loss function by adjusting model parameters iteratively.\n",
    "  - Provides control over batch size and learning rate, allowing for fine-grained adjustments in optimization performance.\n",
    "\n",
    "**When to Use:**\n",
    "- Use SGD when dealing with:\n",
    "  - **Large Datasets**: By processing batches of data sequentially, SGD efficiently handles large datasets by updating model parameters iteratively.\n",
    "  - **Convex Optimization**: Optimizing convex loss functions where the objective is to find a global minimum using gradient descent methods.\n",
    "  - **Fine-tuning**: Adjusting learning rate and batch size parameters allows for fine-tuning optimization performance based on computational resources and dataset characteristics."
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a85914",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1259244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> SGD\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_sgd(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386b2281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4367831349372864\n",
      "Epoch 1000, Loss: 0.26773083209991455\n",
      "Epoch 2000, Loss: 0.24346798658370972\n",
      "Epoch 3000, Loss: 0.2361685037612915\n",
      "Epoch 4000, Loss: 0.22949662804603577\n",
      "Epoch 5000, Loss: 0.2232733517885208\n",
      "Epoch 6000, Loss: 0.2173878252506256\n",
      "Epoch 7000, Loss: 0.21178199350833893\n",
      "Epoch 8000, Loss: 0.20645004510879517\n",
      "Epoch 9000, Loss: 0.2014194130897522\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_sgd(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c7dbf",
   "metadata": {},
   "source": [
    "# 2] RMSProp (Root Mean Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460bdbb",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Effective for training recurrent neural networks (RNNs) and other sequences where gradients can vary significantly.\n",
    "  - Adaptively scales the learning rate based on the magnitude of recent gradients for each parameter, which can accelerate convergence in non-convex optimization settings.\n",
    "  - Handles sparse gradients well because of its adaptive learning rate mechanism.\n",
    "\n",
    "**When to Use:**\n",
    "- Use RMSprop when dealing with:\n",
    "  - **Recurrent Neural Networks (RNNs)**: Helps stabilize and accelerate training due to varying gradients over time.\n",
    "  - **Non-convex Optimization**: Adjusts learning rates individually for each parameter based on their recent gradient history, potentially leading to faster convergence."
   ]
  },
  {
   "cell_type": "raw",
   "id": "68662613",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8fcd16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> RMSProp\n",
    "optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_rms(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae13b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4640316963195801\n",
      "Epoch 1000, Loss: 0.09084735810756683\n",
      "Epoch 2000, Loss: 0.0029399986378848553\n",
      "Epoch 3000, Loss: 0.00020975276129320264\n",
      "Epoch 4000, Loss: 9.567279630573466e-05\n",
      "Epoch 5000, Loss: 6.094207492424175e-05\n",
      "Epoch 6000, Loss: 4.440503835212439e-05\n",
      "Epoch 7000, Loss: 3.479731094557792e-05\n",
      "Epoch 8000, Loss: 2.854119702533353e-05\n",
      "Epoch 9000, Loss: 2.415350900264457e-05\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_rms(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26776677",
   "metadata": {},
   "source": [
    "# 3] Adagrad (Adaptive Gradient Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc90c95",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Well-suited for sparse data or problems with features that rarely occur, as it adapts the learning rate based on the cumulative historical squared gradients for each parameter.\n",
    "  - Automatically reduces the learning rate for parameters that have large gradients, which can lead to improved convergence especially in settings with sparse data.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Adagrad when dealing with:\n",
    "  - **Sparse Data**: Features that are rare or have infrequent occurrences, where adaptive learning rates based on historical gradients can be beneficial.\n",
    "  - **Feature Selection**: Automatically adjusts learning rates based on the data's characteristics, potentially reducing the need for manual tuning.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a95469c7",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23c3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> Adagrad\n",
    "optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_adagrad(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259ec293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.30742931365966797\n",
      "Epoch 1000, Loss: 0.25299394130706787\n",
      "Epoch 2000, Loss: 0.2415032535791397\n",
      "Epoch 3000, Loss: 0.23256665468215942\n",
      "Epoch 4000, Loss: 0.22474023699760437\n",
      "Epoch 5000, Loss: 0.21754717826843262\n",
      "Epoch 6000, Loss: 0.21082176268100739\n",
      "Epoch 7000, Loss: 0.20454193651676178\n",
      "Epoch 8000, Loss: 0.1987500786781311\n",
      "Epoch 9000, Loss: 0.19349640607833862\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_adagrad(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcdfa9",
   "metadata": {},
   "source": [
    "# 4] Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7055254",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Combines the benefits of two other popular optimizers, RMSprop and AdaGrad, by using adaptive learning rates for each parameter.\n",
    "  - Well-suited for a wide range of optimization problems in deep learning due to its adaptive nature and efficient convergence properties.\n",
    "  - Handles sparse gradients effectively, making it suitable for problems with noisy data or complex models.\n",
    "  - Automatically adjusts learning rates during training, which can accelerate convergence and improve model performance.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Adam optimizer when dealing with:\n",
    "  - **Deep Learning Models**: Particularly effective for training deep neural networks across various architectures (CNNs, RNNs, etc.).\n",
    "  - **Large-Scale Datasets**: Efficiently handles large datasets by dynamically adapting learning rates based on gradient statistics.\n",
    "  - **Complex Optimization Landscapes**: Benefits from adaptive learning rates and momentum, making it suitable for non-convex optimization problems where finding an optimal solution is challenging."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cc376d8",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9793ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> Adam\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_adam(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce224c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2594316005706787\n",
      "Epoch 1000, Loss: 0.11361584067344666\n",
      "Epoch 2000, Loss: 0.004071903880685568\n",
      "Epoch 3000, Loss: 0.0009629479027353227\n",
      "Epoch 4000, Loss: 0.0003769979521166533\n",
      "Epoch 5000, Loss: 0.00017743752687238157\n",
      "Epoch 6000, Loss: 9.130692342296243e-05\n",
      "Epoch 7000, Loss: 4.93674524477683e-05\n",
      "Epoch 8000, Loss: 2.749638952082023e-05\n",
      "Epoch 9000, Loss: 1.5607356544933282e-05\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_adam(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2d61b",
   "metadata": {},
   "source": [
    "# 5] N-Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c7335",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Integrates Nesterov's accelerated gradient (NAG) into Adam optimizer, combining the benefits of both Nesterov momentum and adaptive learning rates.\n",
    "  - Converges faster than traditional Adam and exhibits better performance on many deep learning tasks.\n",
    "  - Robust to noisy gradients and sparse data, similar to Adam, due to its adaptive learning rate mechanism.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Nadam optimizer when:\n",
    "  - **Deep Learning Models**: Suitable for a wide range of deep learning architectures, including CNNs, RNNs, and transformers.\n",
    "  - **Fast Convergence**: Particularly effective for models requiring fast convergence and stable optimization, especially in scenarios with large-scale datasets.\n",
    "  - **Regularization**: Works well with regularization techniques like dropout and weight decay, enhancing generalization and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fd48b2d",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb8e4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> Nadam\n",
    "optimizer=tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_nadam(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1b45fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.33062759041786194\n",
      "Epoch 1000, Loss: 0.1296459436416626\n",
      "Epoch 2000, Loss: 0.12607485055923462\n",
      "Epoch 3000, Loss: 0.1254422962665558\n",
      "Epoch 4000, Loss: 0.12521952390670776\n",
      "Epoch 5000, Loss: 0.12511873245239258\n",
      "Epoch 6000, Loss: 0.12506718933582306\n",
      "Epoch 7000, Loss: 0.1250390261411667\n",
      "Epoch 8000, Loss: 0.1250230073928833\n",
      "Epoch 9000, Loss: 0.12501367926597595\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_nadam(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35038f6",
   "metadata": {},
   "source": [
    "# 6] Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c21d2",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Extension of Adam that introduces an alternative approach to compute the adaptive learning rates based on the infinity norm (maximum norm) of the gradients.\n",
    "  - Particularly effective for models with parameters that exhibit large variance in gradients, as it provides a more stable update rule.\n",
    "  - Maintains the benefits of Adam in terms of adaptive learning rates and momentum while simplifying the computation compared to Adam.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Adamax optimizer when:\n",
    "  - **Dealing with Large Models**: Especially useful for models with parameters that vary significantly in their gradient magnitudes.\n",
    "  - **Natural Language Processing (NLP)**: Well-suited for training models like recurrent neural networks (RNNs) and transformers in NLP tasks due to their large parameter space and varied gradients.\n",
    "  - **Sparse Data**: Efficiently handles sparse gradients, making it suitable for models trained on data with irregular or missing information."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b6ed33b",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf7fc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> Adamax\n",
    "optimizer=tf.keras.optimizers.Adamax(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_adamax(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85cc4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3066166341304779\n",
      "Epoch 1000, Loss: 0.1298677623271942\n",
      "Epoch 2000, Loss: 0.05104735121130943\n",
      "Epoch 3000, Loss: 0.003056046087294817\n",
      "Epoch 4000, Loss: 0.0005572356749325991\n",
      "Epoch 5000, Loss: 0.0001495684264227748\n",
      "Epoch 6000, Loss: 4.6029505028855056e-05\n",
      "Epoch 7000, Loss: 1.5061059457366355e-05\n",
      "Epoch 8000, Loss: 5.09380743096699e-06\n",
      "Epoch 9000, Loss: 1.7624225847612252e-06\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_adamax(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe0f14",
   "metadata": {},
   "source": [
    "# 7] Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d03aa7",
   "metadata": {},
   "source": [
    "**Use Case:**\n",
    "- **Advantages:**\n",
    "  - Adaptively adjusts learning rates based on a moving average of gradient updates without the need for a manually specified global learning rate.\n",
    "  - Particularly effective in scenarios where setting a fixed learning rate is challenging or impractical.\n",
    "  - Robust to noisy gradients and sparse data due to its adaptive learning rate mechanism.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Adadelta optimizer when:\n",
    "  - **Noisy or Sparse Data**: Handles data with irregularities or missing information effectively by dynamically adjusting learning rates.\n",
    "  - **Complex Optimization Landscapes**: Well-suited for non-convex optimization problems where the gradient magnitudes vary widely across parameters.\n",
    "  - **Long-Term Dependencies**: Effective for recurrent neural networks (RNNs) and other models with long-term dependencies, as it mitigates the vanishing/exploding gradient problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6161b851",
   "metadata": {},
   "source": [
    "Same Eaxmple only changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3f41327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent var\n",
    "x_data=tf.constant([\n",
    "    [0.0,0.0],\n",
    "    [0.0,0.1],\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "# Dependent var\n",
    "y_data=tf.constant([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_size=2  # two inputs i.e 0.0 and i.0\n",
    "hidden_size=3 #hidden layer has 3neurons\n",
    "output_size=1  #since it is classification o/p is either 1.0 r 0.0\n",
    "\n",
    "\n",
    "#random normalized initialization of weights and biases\n",
    "W1=tf.Variable(tf.random.normal([input_size,hidden_size]))\n",
    "b1=tf.Variable(tf.random.normal([hidden_size]))\n",
    "W2=tf.Variable(tf.random.normal([hidden_size,output_size]))\n",
    "b2=tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "\n",
    "#forward pass\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    #using sigmoid\n",
    "    a1=tf.nn.sigmoid(z1)\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.sigmoid(z2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "\n",
    "#loss function\n",
    "@tf.function\n",
    "def compute_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    return tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#optimizer --> Adadelta\n",
    "optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.01)\n",
    "\n",
    "#training phase\n",
    "@tf.function\n",
    "def train_step_adadelta(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass\n",
    "        y_pred=forward_pass(x)\n",
    "        #loss\n",
    "        loss=compute_loss(y,y_pred)\n",
    "    gradients=tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,[W1,b1,W2,b2]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d263b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4338855445384979\n",
      "Epoch 1000, Loss: 0.42960119247436523\n",
      "Epoch 2000, Loss: 0.4228862524032593\n",
      "Epoch 3000, Loss: 0.4137146472930908\n",
      "Epoch 4000, Loss: 0.40178748965263367\n",
      "Epoch 5000, Loss: 0.38677048683166504\n",
      "Epoch 6000, Loss: 0.3685241937637329\n",
      "Epoch 7000, Loss: 0.3474231958389282\n",
      "Epoch 8000, Loss: 0.3246522545814514\n",
      "Epoch 9000, Loss: 0.3022427260875702\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step_adadelta(x_data, y_data)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747f823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6aff637e",
   "metadata": {},
   "source": [
    "For same data we compared the preformance of optmizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1e173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
